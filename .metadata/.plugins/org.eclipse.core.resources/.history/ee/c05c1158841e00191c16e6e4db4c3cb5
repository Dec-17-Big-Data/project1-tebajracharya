package ques2;

import java.io.IOException;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

/**
* Input is a comma-separated string, interpreted as Key:Value. 
* Output is Key:Value, and the value contains the Country as key and percentage of graduates as the value.
*/

public class FemaleEducationMapper extends Mapper<LongWritable, Text, Text, DoubleWritable>{
	/**
	 * The map function gets a key which is a byte offset and value is a single line from the csv file.
	 * It uses a regex to filter out the search criteria and removes punctuation and stored in list of string by words.
	 * Each word is parsed from the rear to get the most recent percentage and the output is written back to the HDFS along
	 * with the key as the name of the country.
	 */
	@Override
    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		if (value.toString().contains("SE.TER.CUAT.BA.FE.ZS") && value.toString().contains("United States")) ||
			(value.toString().contains("SE.TER.ENRR.FE") && value.toString().contains("United States"))
			{
			String remQuote = value.toString().replace("\"", "");			
			String line[] = remQuote.split(",");
			Double toMostCurrentPercentage = 0.0;
			Double fromNeededPercentage = 0.0;
			Double percentageIncreased = 0.0;
				for(int i = line.length - 1; i > 0; i--) {
					try{
						toMostCurrentPercentage = Double.parseDouble(line[i]);	
					}
					catch(Exception e) {
					}
				}
				for(int i = 44;i < line.length - 1; i++) {
					try{
						fromNeededPercentage = Double.parseDouble(line[i]);	
					}
					catch(Exception e) {
					}
				}
				percentageIncreased = (toMostCurrentPercentage/fromNeededPercentage) * 100;
				context.write(new Text(line[0].toString()), new DoubleWritable(percentageIncreased));	
		}
	}	
}
